{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import necessary packages and libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization,Input, LSTM\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions of imported packages:\n",
      "Num GPUs Available:  1\n",
      "numpy: 1.24.3\n",
      "pandas: 2.0.3\n",
      "seaborn: 0.13.2\n",
      "matplotlib: 3.7.2\n",
      "tensorflow: 2.10.0\n",
      "scikit-learn: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Version information\n",
    "print(\"Versions of imported packages:\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"seaborn: {sns.__version__}\")\n",
    "print(f\"matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"tensorflow: {tf.__version__}\")\n",
    "print(f\"scikit-learn: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (100000, 28, 28)\n",
      "Training labels: (100000,)\n",
      "Test data: (20000, 28, 28)\n",
      "Test labels: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "with open('data/emnist_train.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('data/emnist_test.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "x_train = train_data['data']\n",
    "y_train = train_data['labels']\n",
    "\n",
    "x_test = test_data['data']\n",
    "y_test = test_data['labels']\n",
    "\n",
    "print(\"Training data:\", x_train.shape)\n",
    "print(\"Training labels:\", y_train.shape)\n",
    "print(\"Test data:\", x_test.shape)\n",
    "print(\"Test labels:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some data\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))  # 2 rows and 5 columns displaying 10 images\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_train[i], cmap='gray')  # Display the ith image, using greyscale colour mapping\n",
    "    ax.set_title(f\"Label: {y_train[i]}\")  # Showing corresponding tags\n",
    "    ax.axis('off')  # no axes\n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x_train, y_train, x_test, y_test, num_classes=62, validation_split=0.2, one_hot_encode=True):\n",
    "\n",
    "    # Reshape training and test data for grayscale: (num_samples, 28, 28, 1)\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32')\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32')\n",
    "    \n",
    "    # Normalize the training and test data \n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "    \n",
    "    # Optional One-hot coded labels\n",
    "    if one_hot_encode:\n",
    "        y_train = to_categorical(y_train, num_classes)\n",
    "        y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "    # split the data into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=validation_split, random_state=42)\n",
    "    \n",
    "    # show the results\n",
    "    print(\"\\nAfter preprocessing:\")\n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"x_val shape: {x_val.shape}\")\n",
    "    print(f\"y_val shape: {y_val.shape}\")\n",
    "    print(f\"x_test shape: {x_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMNIST has 62 classes\n",
    "num_classes=62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After preprocessing:\n",
      "x_train shape: (80000, 28, 28, 1)\n",
      "y_train shape: (80000, 62)\n",
      "x_val shape: (20000, 28, 28, 1)\n",
      "y_val shape: (20000, 62)\n",
      "x_test shape: (20000, 28, 28, 1)\n",
      "y_test shape: (20000, 62)\n"
     ]
    }
   ],
   "source": [
    "# Getting pre-processed data\n",
    "x_train_cnn, y_train_cnn, x_val_cnn, y_val_cnn, x_test_cnn, y_test_cnn = preprocess_data(x_train, y_train, x_test, y_test, one_hot_encode=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data augmentation generator is ready to use during model training.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation setup\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "# Fit the datagen on the training data\n",
    "datagen.fit(x_train_cnn)\n",
    "\n",
    "print(\"\\nData augmentation generator is ready to use during model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 13, 13, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 5, 5, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               204928    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 62)                7998      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 232,126\n",
      "Trainable params: 231,934\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Implementation\n",
    "\n",
    "model_cnn = Sequential()\n",
    "\n",
    "# Convolutional Layer 1\n",
    "model_cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn.add(BatchNormalization())\n",
    "\n",
    "# Convolutional Layer 2\n",
    "model_cnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn.add(BatchNormalization())\n",
    "\n",
    "# Flatten and Dense Layers\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(128, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_cnn_model(optimizer='adam', dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F0rR1ver\\AppData\\Local\\Temp\\ipykernel_30828\\890626091.py:3: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model_cnn_cv = KerasClassifier(build_fn=create_cnn_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.842212 using {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 5, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for the CNN model\n",
    "# Wrap the model using the function you have created\n",
    "model_cnn_cv = KerasClassifier(build_fn=create_cnn_model, verbose=0)\n",
    "\n",
    "# Define the grid search parameters\n",
    "param_grid = {\n",
    "    'batch_size': [64, 128],\n",
    "    'epochs': [5],  # Set to a low number for demonstration purposes\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'dropout_rate': [0.3, 0.5]\n",
    "}\n",
    "\n",
    "# Create Grid Search\n",
    "grid_cnn = GridSearchCV(estimator=model_cnn_cv, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "\n",
    "# Fit the grid search\n",
    "# Note: This process will take a considerable amount of time\n",
    "grid_result_cnn = grid_cnn.fit(x_train_cnn, y_train_cnn)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_cnn.best_score_, grid_result_cnn.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 5, 'optimizer': 'adam'}\n",
      "Epoch 1/10\n",
      "563/563 [==============================] - 2s 3ms/step - loss: 0.9707 - accuracy: 0.7207 - val_loss: 1.0317 - val_accuracy: 0.6676\n",
      "Epoch 2/10\n",
      "563/563 [==============================] - 2s 3ms/step - loss: 0.5734 - accuracy: 0.8078 - val_loss: 0.4894 - val_accuracy: 0.8304\n",
      "Epoch 3/10\n",
      "563/563 [==============================] - 2s 3ms/step - loss: 0.4854 - accuracy: 0.8303 - val_loss: 0.4533 - val_accuracy: 0.8443\n",
      "Epoch 4/10\n",
      "563/563 [==============================] - 2s 3ms/step - loss: 0.4445 - accuracy: 0.8420 - val_loss: 0.4465 - val_accuracy: 0.8443\n",
      "Epoch 5/10\n",
      "563/563 [==============================] - 2s 3ms/step - loss: 0.4164 - accuracy: 0.8479 - val_loss: 0.4529 - val_accuracy: 0.8438\n",
      "Epoch 6/10\n",
      "563/563 [==============================] - 2s 3ms/step - loss: 0.3907 - accuracy: 0.8565 - val_loss: 0.4432 - val_accuracy: 0.8471\n",
      "Epoch 7/10\n",
      "563/563 [==============================] - 1s 3ms/step - loss: 0.3706 - accuracy: 0.8608 - val_loss: 0.4447 - val_accuracy: 0.8518\n",
      "Epoch 8/10\n",
      "563/563 [==============================] - 2s 3ms/step - loss: 0.3553 - accuracy: 0.8650 - val_loss: 0.4500 - val_accuracy: 0.8482\n",
      "Epoch 9/10\n",
      "563/563 [==============================] - 2s 3ms/step - loss: 0.3411 - accuracy: 0.8699 - val_loss: 0.4855 - val_accuracy: 0.8425\n",
      "Epoch 10/10\n",
      "563/563 [==============================] - 2s 3ms/step - loss: 0.3262 - accuracy: 0.8739 - val_loss: 0.4644 - val_accuracy: 0.8469\n"
     ]
    }
   ],
   "source": [
    "# Extract the best parameters\n",
    "best_params_cnn = grid_result_cnn.best_params_\n",
    "print(\"Best Parameters:\", best_params_cnn)\n",
    "\n",
    "# Create a new model with the best parameters\n",
    "model_cnn_best = create_cnn_model(optimizer=best_params_cnn['optimizer'], dropout_rate=best_params_cnn['dropout_rate'])\n",
    "\n",
    "# Train the model with the training data\n",
    "history_cnn = model_cnn_best.fit(\n",
    "    x_train_cnn, y_train_cnn,\n",
    "    batch_size=best_params_cnn['batch_size'],\n",
    "    epochs=10,  # Increase the number of epochs for better performance\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After preprocessing:\n",
      "x_train shape: (80000, 28, 28, 1)\n",
      "y_train shape: (80000, 62)\n",
      "x_val shape: (20000, 28, 28, 1)\n",
      "y_val shape: (20000, 62)\n",
      "x_test shape: (20000, 28, 28, 1)\n",
      "y_test shape: (20000, 62)\n"
     ]
    }
   ],
   "source": [
    "# Getting pre-processed data\n",
    "x_train_rnn, y_train_rnn, x_val_rnn, y_val_rnn, x_test_rnn, y_test_rnn = preprocess_data(x_train, y_train, x_test, y_test, one_hot_encode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design RNN Model\n",
    "def create_rnn_model(learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(28, 28)))\n",
    "    # LSTM\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    # Dense\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    # Output\n",
    "    model.add(Dense(62, activation='softmax'))\n",
    "    # Compile\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F0rR1ver\\AppData\\Local\\Temp\\ipykernel_30828\\1422443469.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  rnn_model = KerasClassifier(build_fn=create_rnn_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] END .......batch_size=64, epochs=5, learning_rate=0.001; total time=  22.6s\n",
      "[CV] END .......batch_size=64, epochs=5, learning_rate=0.001; total time=  22.0s\n",
      "[CV] END .......batch_size=64, epochs=5, learning_rate=0.001; total time=  22.0s\n",
      "[CV] END ......batch_size=64, epochs=5, learning_rate=0.0001; total time=  22.2s\n",
      "[CV] END ......batch_size=64, epochs=5, learning_rate=0.0001; total time=  22.4s\n",
      "[CV] END ......batch_size=64, epochs=5, learning_rate=0.0001; total time=  22.1s\n",
      "[CV] END ......batch_size=128, epochs=5, learning_rate=0.001; total time=  12.4s\n",
      "[CV] END ......batch_size=128, epochs=5, learning_rate=0.001; total time=  12.7s\n",
      "[CV] END ......batch_size=128, epochs=5, learning_rate=0.001; total time=  12.5s\n",
      "[CV] END .....batch_size=128, epochs=5, learning_rate=0.0001; total time=  12.4s\n",
      "[CV] END .....batch_size=128, epochs=5, learning_rate=0.0001; total time=  12.6s\n",
      "[CV] END .....batch_size=128, epochs=5, learning_rate=0.0001; total time=  12.4s\n",
      "best params: {'batch_size': 64, 'epochs': 5, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "rnn_model = KerasClassifier(build_fn=create_rnn_model, verbose=0)\n",
    "param_grid_rnn = {\n",
    "    'batch_size': [64, 128],\n",
    "    'epochs': [5],\n",
    "    'learning_rate': [0.001, 0.0001],\n",
    "}\n",
    "grid_search_rnn = GridSearchCV(estimator=rnn_model, param_grid=param_grid_rnn, cv=3, verbose=2, n_jobs=1)\n",
    "grid_result_rnn = grid_search_rnn.fit(x_train_rnn, y_train_rnn)\n",
    "print(f\"best params: {grid_result_rnn.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_13 (LSTM)              (None, 128)               80384     \n",
      "                                                                 \n",
      " batch_normalization_117 (Ba  (None, 128)              512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_78 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_79 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 62)                7998      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105,406\n",
      "Trainable params: 105,150\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 1.7462 - accuracy: 0.5343 - val_loss: 0.9014 - val_accuracy: 0.7164\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.9369 - accuracy: 0.7146 - val_loss: 0.6727 - val_accuracy: 0.7790\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7735 - accuracy: 0.7586 - val_loss: 0.5831 - val_accuracy: 0.7950\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.6974 - accuracy: 0.7797 - val_loss: 0.5662 - val_accuracy: 0.8031\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.6459 - accuracy: 0.7941 - val_loss: 0.5372 - val_accuracy: 0.8163\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6107 - accuracy: 0.8022 - val_loss: 0.5296 - val_accuracy: 0.8176\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.5843 - accuracy: 0.8109 - val_loss: 0.5167 - val_accuracy: 0.8245\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.5644 - accuracy: 0.8145 - val_loss: 0.5070 - val_accuracy: 0.8245\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.5445 - accuracy: 0.8208 - val_loss: 0.5087 - val_accuracy: 0.8275\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 7s 5ms/step - loss: 0.5304 - accuracy: 0.8248 - val_loss: 0.5005 - val_accuracy: 0.8292\n"
     ]
    }
   ],
   "source": [
    "# best_params_rnn = grid_result_rnn.best_params_\n",
    "best_params_rnn = {'batch_size': 64, 'epochs': 10, 'model__learning_rate': 0.001}\n",
    "best_rnn = create_rnn_model(learning_rate=best_params_rnn['model__learning_rate'])\n",
    "best_rnn.summary()\n",
    "history_rnn = best_rnn.fit(x_train_rnn, y_train_rnn,\n",
    "                          validation_data=(x_val_rnn, y_val_rnn),\n",
    "                          epochs=best_params_rnn['epochs'],\n",
    "                          batch_size=best_params_rnn['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After preprocessing:\n",
      "x_train shape: (80000, 28, 28, 1)\n",
      "y_train shape: (80000,)\n",
      "x_val shape: (20000, 28, 28, 1)\n",
      "y_val shape: (20000,)\n",
      "x_test shape: (20000, 28, 28, 1)\n",
      "y_test shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# Getting pre-processed data\n",
    "x_train_vit, y_train_vit, x_val_vit, y_val_vit, x_test_vit, y_test_vit = preprocess_data(x_train, y_train, x_test, y_test, one_hot_encode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation vision transformer\n",
    "class vit(Model):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, dropout):\n",
    "        super(vit, self).__init__()\n",
    "\n",
    "        # Calculate the total number of patches in the image\n",
    "        # If the image size is 28x28 and the patch size is 4, then num_patches = (28 // 4) ** 2 = 49\n",
    "        assert image_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = patch_size * patch_size  # Number of pixels per patch\n",
    "\n",
    "        # Patch embedding layer: transforms each patch into a dim-dimensional feature vector\n",
    "        self.patch_embedding = layers.Dense(dim)\n",
    "        \n",
    "        # Location embedding for learning the relative position of each patch in the image\n",
    "        self.position_embedding = layers.Embedding(input_dim=num_patches + 1, output_dim=dim)\n",
    "        \n",
    "        # Classification token (class token) to indicate the class of the image; randomly initialised to a trainable variable\n",
    "        self.cls_token = tf.Variable(tf.zeros([1, 1, dim]), trainable=True)\n",
    "\n",
    "        # Transformer coding layers\n",
    "        # Each transformer layer contains a MultiHeadAttention mechanism, a Layer Normalisation Layer and an MLP Layer\n",
    "        # layers.MultiHeadAttention performs multi-head self-attention computation\n",
    "        self.transformer_layers = [\n",
    "            layers.MultiHeadAttention(num_heads=heads, key_dim=dim, dropout=dropout) for _ in range(depth)\n",
    "        ]\n",
    "        \n",
    "        # Layer normalisation before each transformer layer\n",
    "        self.norm_layers = [layers.LayerNormalization(epsilon=1e-6) for _ in range(depth)]\n",
    "        \n",
    "        # Small MLP networks behind each transformer layer\n",
    "        self.mlp_layers = [tf.keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation='relu'),  # Hidden Layers for MLP\n",
    "            layers.Dropout(dropout),                   # Dropout prevents overfitting\n",
    "            layers.Dense(dim),                         # Output to dim dimension for easy residual linking\n",
    "            layers.Dropout(dropout)\n",
    "        ]) for _ in range(depth)]\n",
    "\n",
    "        # Category header: output scores for each category\n",
    "        self.mlp_head = tf.keras.Sequential([\n",
    "            layers.LayerNormalization(epsilon=1e-6),   # Layer normalisation before output\n",
    "            layers.Dense(num_classes)                  # Output layer with dimension num_classes\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        # Get batch size\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        # Divide the input image into patches\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=x,\n",
    "            sizes=[1, 4, 4, 1],          \n",
    "            strides=[1, 4, 4, 1],        \n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "        patches = tf.reshape(patches, [batch_size, -1, 4 * 4])  # Flatten each patch into one dimension\n",
    "\n",
    "        # Apply patch embedding to transform each patch into a dim-dimensional feature vector\n",
    "        x = self.patch_embedding(patches)\n",
    "\n",
    "        # Extend the size of the classification markers to the batch\n",
    "        cls_tokens = tf.repeat(self.cls_token, repeats=batch_size, axis=0)\n",
    "        \n",
    "        # Splice classification markers in front of patch features\n",
    "        x = tf.concat([cls_tokens, x], axis=1)\n",
    "        \n",
    "        # Add positional embedding to indicate the relative position of each patch\n",
    "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
    "        x += self.position_embedding(positions)\n",
    "\n",
    "        # Transformer encoder part\n",
    "        # Each transformer layer undergoes a layer of normalisation, a multi-head attention mechanism, and an MLP layer\n",
    "        for norm_layer, transformer_layer, mlp_layer in zip(self.norm_layers, self.transformer_layers, self.mlp_layers):\n",
    "            x = norm_layer(x)\n",
    "            x = transformer_layer(x, x) + x\n",
    "            x = mlp_layer(x) + x\n",
    "\n",
    "        # Classification predictions using only classified labelled features\n",
    "        x = x[:, 0]  \n",
    "        return self.mlp_head(x)  \n",
    "    \n",
    "# Reference:\n",
    "# https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/\n",
    "# https://blog.csdn.net/qq_37541097/article/details/118242600\n",
    "# https://github.com/WZMIAOMIAO/deep-learning-for-image-processing?tab=readme-ov-file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Vision Transformer model and compile it.\n",
    "def build_vit_model(image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, dropout):\n",
    "    model = vit(image_size=image_size, \n",
    "                patch_size=patch_size, \n",
    "                num_classes=num_classes, \n",
    "                dim=dim, \n",
    "                depth=depth, \n",
    "                heads=heads, \n",
    "                mlp_dim=mlp_dim, \n",
    "                dropout=dropout)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Reference:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.6962 - accuracy: 0.5297 - val_loss: 0.9745 - val_accuracy: 0.6914\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8824 - accuracy: 0.7218 - val_loss: 0.7341 - val_accuracy: 0.7558\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7293 - accuracy: 0.7622 - val_loss: 0.6431 - val_accuracy: 0.7834\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 0.6610 - accuracy: 0.7789 - val_loss: 0.5986 - val_accuracy: 0.7959\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 0.6166 - accuracy: 0.7910 - val_loss: 0.5891 - val_accuracy: 0.8002\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 0.5898 - accuracy: 0.7994 - val_loss: 0.5534 - val_accuracy: 0.8086\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.5580 - accuracy: 0.8069 - val_loss: 0.5446 - val_accuracy: 0.8105\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.5429 - accuracy: 0.8123 - val_loss: 0.5118 - val_accuracy: 0.8192\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.5288 - accuracy: 0.8158 - val_loss: 0.5481 - val_accuracy: 0.8092\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.5162 - accuracy: 0.8186 - val_loss: 0.5146 - val_accuracy: 0.8203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2499e0f46a0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model testing with given parameters\n",
    "test_model_vit = build_vit_model(image_size=28, patch_size=4, num_classes=num_classes, dim=64, depth=4, heads=4, mlp_dim=128, dropout=0.1)\n",
    "\n",
    "test_model_vit.fit(x_train_vit, y_train_vit, batch_size=64, epochs=10, validation_data=(x_val_vit, y_val_vit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 16s 7ms/step - loss: 0.4356 - accuracy: 0.8430\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.5005 - accuracy: 0.8255\n",
      "Train Loss: 0.4356, Train Accuracy: 0.8430\n",
      "Validation Loss: 0.5005, Validation Accuracy: 0.8255\n"
     ]
    }
   ],
   "source": [
    "# Display the results of the test\n",
    "train_loss_test_vit, train_accuracy_test_vit = test_model_vit.evaluate(x_train_vit, y_train_vit)\n",
    "val_loss_test_vit, val_accuracy_test_vit = test_model_vit.evaluate(x_val_vit, y_val_vit)\n",
    "\n",
    "print(f\"Train Loss: {train_loss_test_vit:.4f}, Train Accuracy: {train_accuracy_test_vit:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss_test_vit:.4f}, Validation Accuracy: {val_accuracy_test_vit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F0rR1ver\\AppData\\Local\\Temp\\ipykernel_30828\\1204293435.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model_vit = KerasClassifier(build_fn=build_vit_model, epochs=5, batch_size=64, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# Wrapping the build_vit_model function with KerasClassifier for compatibility with scikit-learn's RandomisedSearchCV\n",
    "model_vit = KerasClassifier(build_fn=build_vit_model, epochs=5, batch_size=64, verbose=0)\n",
    "\n",
    "# Defining the Hyperparameter\n",
    "param_dist_vit = {\n",
    "    'image_size': [28],\n",
    "    'patch_size': [2, 4, 7],\n",
    "    'num_classes':[62],\n",
    "    'dim': [32, 64, 128],\n",
    "    'depth': [4, 6, 8],\n",
    "    'heads': [4, 8],\n",
    "    'mlp_dim': [64, 128, 256],\n",
    "    'dropout': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Use Randomized Search + Cross-Validation\n",
    "random_search_vit = RandomizedSearchCV(\n",
    "    estimator=model_vit,\n",
    "    param_distributions=param_dist_vit,\n",
    "    n_iter=10,\n",
    "    scoring='accuracy',\n",
    "    cv=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834/834 [==============================] - 9s 10ms/step\n",
      "834/834 [==============================] - 8s 9ms/step\n",
      "834/834 [==============================] - 8s 9ms/step\n",
      "834/834 [==============================] - 7s 8ms/step\n",
      "834/834 [==============================] - 7s 8ms/step\n",
      "834/834 [==============================] - 7s 7ms/step\n",
      "834/834 [==============================] - 6s 7ms/step\n",
      "834/834 [==============================] - 6s 7ms/step\n",
      "834/834 [==============================] - 6s 7ms/step\n",
      "834/834 [==============================] - 8s 9ms/step\n",
      "834/834 [==============================] - 8s 9ms/step\n",
      "834/834 [==============================] - 8s 9ms/step\n",
      "834/834 [==============================] - 5s 5ms/step\n",
      "834/834 [==============================] - 5s 5ms/step\n",
      "834/834 [==============================] - 5s 5ms/step\n",
      "834/834 [==============================] - 8s 9ms/step\n",
      "834/834 [==============================] - 8s 9ms/step\n",
      "834/834 [==============================] - 8s 9ms/step\n",
      "834/834 [==============================] - 5s 6ms/step\n",
      "834/834 [==============================] - 5s 6ms/step\n",
      "834/834 [==============================] - 5s 6ms/step\n",
      "834/834 [==============================] - 7s 8ms/step\n",
      "834/834 [==============================] - 7s 7ms/step\n",
      "834/834 [==============================] - 7s 8ms/step\n",
      "834/834 [==============================] - 5s 5ms/step\n",
      "834/834 [==============================] - 5s 5ms/step\n",
      "834/834 [==============================] - 5s 5ms/step\n",
      "834/834 [==============================] - 7s 8ms/step\n",
      "834/834 [==============================] - 7s 8ms/step\n",
      "834/834 [==============================] - 7s 8ms/step\n",
      "Best Parameters: {'patch_size': 4, 'num_classes': 62, 'mlp_dim': 256, 'image_size': 28, 'heads': 4, 'dropout': 0.2, 'dim': 128, 'depth': 4}\n",
      "Best Score: 0.7580751247697082\n"
     ]
    }
   ],
   "source": [
    "random_search_vit.fit(x_train_vit, y_train_vit)\n",
    "\n",
    "print(\"Best Parameters:\", random_search_vit.best_params_)\n",
    "print(\"Best Score:\", random_search_vit.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1250/1250 [==============================] - 18s 13ms/step - loss: 1.7103 - accuracy: 0.5199 - val_loss: 0.9820 - val_accuracy: 0.6979\n",
      "Epoch 2/30\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 0.9670 - accuracy: 0.6944 - val_loss: 0.8064 - val_accuracy: 0.7350\n",
      "Epoch 3/30\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 0.8297 - accuracy: 0.7324 - val_loss: 0.7275 - val_accuracy: 0.7541\n",
      "Epoch 4/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7532 - accuracy: 0.7538 - val_loss: 0.6667 - val_accuracy: 0.7769\n",
      "Epoch 5/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.7055 - accuracy: 0.7670 - val_loss: 0.6529 - val_accuracy: 0.7789\n",
      "Epoch 6/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6678 - accuracy: 0.7766 - val_loss: 0.6884 - val_accuracy: 0.7728\n",
      "Epoch 7/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6357 - accuracy: 0.7854 - val_loss: 0.6331 - val_accuracy: 0.7890\n",
      "Epoch 8/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6113 - accuracy: 0.7937 - val_loss: 0.5940 - val_accuracy: 0.7979\n",
      "Epoch 9/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5901 - accuracy: 0.7984 - val_loss: 0.5547 - val_accuracy: 0.8116\n",
      "Epoch 10/30\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5754 - accuracy: 0.8034 - val_loss: 0.5875 - val_accuracy: 0.8013\n",
      "Epoch 11/30\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5686 - accuracy: 0.8028 - val_loss: 0.5526 - val_accuracy: 0.8095\n",
      "Epoch 12/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5508 - accuracy: 0.8094 - val_loss: 0.5264 - val_accuracy: 0.8193\n",
      "Epoch 13/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5358 - accuracy: 0.8134 - val_loss: 0.5297 - val_accuracy: 0.8122\n",
      "Epoch 14/30\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5267 - accuracy: 0.8173 - val_loss: 0.5251 - val_accuracy: 0.8142\n",
      "Epoch 15/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5171 - accuracy: 0.8195 - val_loss: 0.5329 - val_accuracy: 0.8148\n",
      "Epoch 16/30\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5109 - accuracy: 0.8204 - val_loss: 0.5016 - val_accuracy: 0.8247\n",
      "Epoch 17/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.5022 - accuracy: 0.8239 - val_loss: 0.5069 - val_accuracy: 0.8220\n",
      "Epoch 18/30\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4937 - accuracy: 0.8254 - val_loss: 0.4995 - val_accuracy: 0.8253\n",
      "Epoch 19/30\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4878 - accuracy: 0.8263 - val_loss: 0.4908 - val_accuracy: 0.8284\n",
      "Epoch 20/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4802 - accuracy: 0.8292 - val_loss: 0.4676 - val_accuracy: 0.8339\n",
      "Epoch 21/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4757 - accuracy: 0.8305 - val_loss: 0.4842 - val_accuracy: 0.8335\n",
      "Epoch 22/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4722 - accuracy: 0.8309 - val_loss: 0.5037 - val_accuracy: 0.8224\n",
      "Epoch 23/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4647 - accuracy: 0.8334 - val_loss: 0.4828 - val_accuracy: 0.8290\n",
      "Epoch 24/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4642 - accuracy: 0.8331 - val_loss: 0.4768 - val_accuracy: 0.8313\n",
      "Epoch 25/30\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4573 - accuracy: 0.8353 - val_loss: 0.4806 - val_accuracy: 0.8310\n",
      "Epoch 26/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4532 - accuracy: 0.8363 - val_loss: 0.4734 - val_accuracy: 0.8307\n",
      "Epoch 27/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4484 - accuracy: 0.8383 - val_loss: 0.4760 - val_accuracy: 0.8339\n",
      "Epoch 28/30\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.4441 - accuracy: 0.8397 - val_loss: 0.4755 - val_accuracy: 0.8339\n",
      "Epoch 29/30\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 0.4403 - accuracy: 0.8396 - val_loss: 0.4920 - val_accuracy: 0.8253\n",
      "Epoch 30/30\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.4405 - accuracy: 0.8390 - val_loss: 0.4965 - val_accuracy: 0.8243\n"
     ]
    }
   ],
   "source": [
    "best_params_vit = {'patch_size': 4, 'num_classes': 62, 'mlp_dim': 256, 'image_size': 28, 'heads': 4, 'dropout': 0.2, 'dim': 128, 'depth': 4}\n",
    "\n",
    "# Create a new model with the best parameters\n",
    "final_model_vit = build_vit_model(\n",
    "    image_size=best_params_vit['image_size'],\n",
    "    patch_size=best_params_vit['patch_size'],\n",
    "    num_classes=best_params_vit['num_classes'],\n",
    "    dim=best_params_vit['dim'],\n",
    "    depth=best_params_vit['depth'],\n",
    "    heads=best_params_vit['heads'],\n",
    "    mlp_dim=best_params_vit['mlp_dim'],\n",
    "    dropout=best_params_vit['dropout']\n",
    ")\n",
    "\n",
    "# Train the model with the training data\n",
    "history_vit = final_model_vit.fit(\n",
    "    x_train_vit, y_train_vit,\n",
    "    batch_size=64,\n",
    "    epochs=30,  \n",
    "    validation_data=(x_val_vit, y_val_vit)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
